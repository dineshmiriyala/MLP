# Neural Network-Based Text Generation Model

This repository contains a neural network model designed to explore Natural Language Processing (NLP) techniques, specifically leveraging transformer-inspired concepts and multi-layer perceptron (MLP) neural networks for generating conversational text. The model was trained using a Reddit conversations dataset.

## Project Overview
This project focuses on understanding and experimenting with:

- NLP foundations using neural networks.
- Basics of embedding layers, linear layers, batch normalization, activation functions, and loss functions.
- Text generation tasks inspired by transformer architectures.

## Dataset
The dataset used is derived from Reddit conversations. It has been preprocessed and encoded into numerical representations suitable for neural network training.

- **Data Files:**
  - `trainingData.pkl`: Contains the preprocessed training, validation, and test splits.
  - `encode_decode.pkl`: Mapping dictionaries to encode words into integers and decode integers back into words.

## Project Structure
```
├── data
│   ├── encode_decode.pkl     # Encoding and decoding mappings
│   ├── trainingData.pkl      # Training dataset (train, val, test splits)
│   ├── params.pkl            # Model parameters after training
│   └── stats.pkl             # Training statistics (loss values and learning rates)
├── main.py                   # Main execution script
├── model.py                  # Neural network model definition
└── README.md                 # Project documentation
```

## How the Model Works

### Neural Network Architecture
The model architecture consists of:

- **Embedding Layer:** Converts words into dense vector representations.
- **Linear Layers:** Transform embeddings to hidden representations and further to output logits.
- **Batch Normalization:** Ensures stable training.
- **Activation Function:** Non-linear transformations using Tanh.
- **Cross-Entropy Loss:** Measures prediction accuracy.

### Workflow
- **Training:**  
  - Select "Train" from the main menu.
  - Enter the number of iterations.
  - Model parameters are updated using mini-batch gradient descent.

- **Visualization (Loss Graph):**  
  - Plot training loss to visualize learning progress.

- **Text Generation:**  
  - Generate conversational text based on trained embeddings.
  - The generated output mimics conversation structures learned from Reddit data.

## Getting Started

### Prerequisites
- Python 3.8 or higher
- PyTorch, tqdm, matplotlib

### Installation
Run `main.py`; it will automatically install required dependencies:
```bash
python main.py
```

### Usage
```bash
python main.py
```

Choose one of the options when prompted:
```
1: Train
2: Loss graph
3: Generate Text
4: Exit
```

- Select `1` for training.
- Select `2` to plot the loss graph.
- Select `3` to generate conversational text.

## Sample Generated Text

Here are some examples of conversational text generated by the trained model:

```
The there be make STAWP.
Stil I know life acros one single right an after today right the I Iran disagred together.
Life is archaeologists ashole hoping.
Or cant the We DEFINITELY car and bet not.
I was my time Frontiersman particularly as some able theyre of youd.
words people themsevels.
Which was also some fal on and my.
You you kilers games later the beter because birthday around it her that fun in the after natural me.
Get mis specific paroting Brave aspire controling aloted severely Sydney Hmph Domestic transit pencils to.
Because might are as then arent super life just.
```

- **Loss Value:** The loss for this output is approximately **7.533**.

## Technical Details

### Libraries Used
- **PyTorch:** Neural network modeling and training.
- **Matplotlib:** Visualization of loss graphs.
- **tqdm:** Progress bar utility during training loops.

### Files Explained
- **main.py:** Manages the user interface and handles operations such as training and text generation.
- **model.py:** Defines the neural network architecture, training and inference methods, and graph plotting functions.

## Contributing
Feel free to fork the repository and submit pull requests for any improvements or experiments!

## Author
- [Dinesh Miriyala](https://github.com/dineshmiriyala)
